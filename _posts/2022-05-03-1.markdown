---
layout: post
title:  "Amazon Builders' Library. TLDR;"
date:   2022-05-03 19:12:00 +0000
categories: Software Engineering
published: false
---

AWS publishes a collection of PDFs containing tips and tricks for building scalable systems. I trimmed the fat and aimed to summarize each article in two paragraphs. I downloaded the PDFs on March 5th 2022 so any articles posted after that date aren't included.

## Using load shedding to avoid overload - David Yanacek

Tips for not killing your services by overloading them.

- Shed requests which would cause load to get too high and lead to a cascading performance degradation. Don't forget to filter out the latency measurements for rejected requests from your latency measurements of client rpc calls. Also, make sure that the shedding threshold is not set lower than the reactive scaling threshold. If you don't, the reactive scaling will not trigger.

Drop doomed requests by making clients include a timeout hint and when the timeout is reached server-side, kill the request. Make the timeout hint transitive by subtracting the time taken by each service before passing the request downstream. Be aware of clock sync issues there. Similarly, check a request to see if it has been sitting in a queue for a long time, before starting to work on it. Throw it out if it's too old, in order to get to the fresher requests sooner.

Pagination helps to avoid wasted work and makes it easier to estimate the time to respond to a request. Return an iterator or similar instead of a large result. When API's are multi-step and include, for example, `start` and `end` calls, prioritize `end` calls to make sure that clients who began their request are able to finish them.

## Workload isolation using shuffle-sharding - Colm MacCÃ¡rthaigh

Shuffle sharding is a technique to improve the reliability of a fleet of clients who may share common resources or logical dependencies. For example, if you have a number of clients who need work done, and a number of workers, you would like to match clients to workers in a many-to-many fashion such that the failure of an individual worker effects as few clients as possible. Concretely, if you have _n_ clients and _m_ workers and each client requires _k_ workers with _k_ < _m_ then you should should try to make sure that the size of the overlap between the sets of _k_ workers assigned to any two clients is minimized.

![]({{ site.url }}/assets/2022-05-03-1/shuffle_sharding.png)

The image shows a configuration where each client is assigned two workers. Even if the two workers of a single client fail, the other clients will both still have a single functioning worker.

The technique is especially useful if resource failure is correlated to the client. For instance, if the client is a server who comes under DDOS attack then all their workers may fail fast.

[implementation](https://github.com/awslabs/route53-infima)

#### [Amazon Builders' Library](https://aws.amazon.com/builders-library)
