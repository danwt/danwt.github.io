---
layout: post
title:  "Amazon Builder's Library. TLDR;"
date:   2022-03-05 19:12:00 +0000
categories: sweng
published: false
---

AWS publishes a collection of PDFs containing tips and tricks for building scalable systems. I tried to trim the fat and present valuable points from each article in two paragraphs or so. I downloaded the PDFs on March 5th 2022 so any articles posted after that date aren't included.

## Using load shedding to avoid overload - David Yanacek

Tips for not killing your services by overloading them:

Shed requests which would cause load to get too high and lead to a cascading performance degradation. Don't forget to filter out the latency measurements for rejected requests from your latency measurements of client rpc calls. Also, make sure that the shedding threshold is not set lower than the reactive scaling threshold. If you don't, the reactive scaling will not trigger.

Drop doomed requests by making clients include a timeout hint and when the timeout is reached server-side, kill the request. Make the timeout hint transitive by subtracting the time taken by each service before passing the request downstream. Be aware of clock sync issues there. Similarly, check a request to see if it has been sitting in a queue for a long time, before starting to work on it. Throw it out if it's too old, in order to get to the fresher requests sooner.

Pagination helps to avoid wasted work and makes it easier to estimate the time to respond to a request. Return an iterator or similar instead of a large result. When API's are multi-step and include, for example, `start` and `end` calls, prioritize `end` calls to make sure that clients who began their request are able to finish them.

## Workload isolation using shuffle-sharding - Colm MacCárthaigh

Shuffle sharding is a technique to improve the reliability of a fleet of clients who may share common resources or logical dependencies. For example, if you have a number of clients who need work done, and a number of workers, you would like to match clients to workers in a many-to-many fashion such that the failure of an individual worker effects as few clients as possible. Concretely, if you have _n_ clients and _m_ workers and each client requires _k_ workers with _k_ < _m_ then you should should try to make sure that the size of the overlap between the sets of _k_ workers assigned to any two clients is minimized.

![]({{ site.url }}/assets/2022-05-03-1/shuffle_sharding.png){: width="75%" }

The image shows a configuration where each client is assigned two workers. Even if the two workers of a single client fail, the other clients will both still have a single functioning worker.

The technique is especially useful if resource failure is correlated to the client. For instance, if the client is a server who comes under DDOS attack then all their workers may fail fast.

## Timeouts, retries, and backoff with jitter - Marc Brooker

Use capped exponential backoff to avoid overloading a system with retires. You can combine this with a token bucket algorithm for requests. Also, add jitter to any timers in the system, but make it deterministic, not random, in order to not end up in debugging hell.

## Static stability using Availability Zones - Becky Weiss, Mike Furr

Define static stability as being stable in the face of failure, without needing to reactively provision more resources. In practice this means having additional resources on standby such that when a failure occurs, the service can use the reserve resources while the failed resources are replaced.

## Reliability, constant work, and a good cup of coffee - Colm MacCárthaigh

Improve stability by always doing the same (maximum) amount of work instead of varying and scaling up the amount of work. If there isn't much work to be done then create dummy work. The reduced workload variance is more predictable and there will be fewer ways for your system to fail in a cascade. A good example is to have a downstream service read configurations from an upstream service in a loop instead of having the upstream service push a new configuration on changes. The solution requires less logic and savings in dev and maintenance cost cans outweigh the cost of additional work.

## Minimizing correlated failures in distributed systems - Joe Mageramov

Use random jitter across many time based operations to reduce the chance of correlated failures. It's a good idea to use a value such as the server IP address as a seed to help with debugging.

## Making retries safe with idempotent APIs - Malcolm Featonby

For particular services it is worth publishing an idempotency contract. The API contains a `clientToken` parameter, which identifies client requests. The service caches requests and their results and returns cached results in the case that a duplicate `clientToken` is received. Furthermore, the parameters of the requests are stored too, and if the `clientToken` matches but the parameters are different, then the service returns an error. In this way it is assumed that the client made an error if sending two differently parameterized requests with the same token. Amazon found it reasonable to expire cached results after a certain time.

## Leader election in distributed systems - Marc Brooker

Use a distributed lock service to elect a leader. The devil is in the details of course...

## Instrumenting distributed systems for operational visibility - David Yanacek

Verbosely log systems to help you and your clients to debug and anticipate problems. In particular

- Prefer to pass around trace id's to help connect log statements between services.
- Pass around metric aggregation objects to be able to log a comprehensible set of metrics at the end of a processing pipeline.
- Test logging at max throughput and consider logging to different disk partitions, logging to a ramdisk or temporary file system, or writing compressed logs. If writing compressed logs be sure to use a compression algorithm that can handle truncation.
- Rotate logs more frequently in order to spread out the work done to transfer or compress the data and avoid spikes in resource utilization, even if this results in slightly more work overall.
- Use opt-in for logging sensitive data, this prevents the scenario where you accidentally add sensitive data to a log, if using opt-out.

## Implementing health checks - David Yanacek

Ensure servers provide their software version in health checks so that zombie servers, servers that have been disconnected for a long time and then reconnect, are detected. Additionally, beware the black hole phenomenon, where a failed server responds to requests faster than other servers, and thus takes a larger portion of work from the load balancer.

## Going faster with continuous delivery - Mark Mansour

This must be an old article: it gives commonplace CI/CD ideas.

## Fairness in multi-tenant systems - David Yanacek

Use token-bucket like algorithms to quota clients. It can be a good idea to give tenants greater quotas when their provisioned service initially starts up, as the workload may be higher due to downloading updates and installing packages, ect. Using quotas raises the question of how to scale quota tracking services: use consistent hashing to associate quota lookup keys for different clients to different lookup servers. Additionally, use algorithms like Heavy Hitter, Top Talkers and Counting Bloom Filters to set an upper bound on the memory used.

Here are the algorithms:

TODO:

## Ensuring rollback safety during deployments - Sandeep Pokkunuri

Deploy changes to fleets of services using Two Phase Deployment: make two waves of changes. The first wave ensures backwards compatibility with the initial state and forwards compatibility with the changes made in the second wave. That way, the second wave of changes can be rolled back and the system will not suffer any residual damage, as the first wave will have ensured forward compatibility.

For example, if you are upgrading communication to use Json instead of XML then first upgrade servers to be able to read Json, and only then upgrade them to also write Json.

Do test the ability to roll changes forward _and_ backwards, in a production-like test environment beforehand.

## Challenges with distributed systems - Jacob Gabrielson

I skipped it.

## Caching challenges and strategies - Matt Brinkley, Jas Chhabra

You can broadly differentiate caching strategies by cache placement. You could place a standalone (on-box) cache in front of each server of a target service, or you could have a separate caching service (external) in front of the target service. With the on-box approach, you have to deal with cache coherence problems. It's easier to offer consistency using the external approach but it implies maintaining another service.

For all caching solutions, consider using soft and hard time-to-live values. Clients will try to refresh cached values when the soft TTL expires, but can rely on reading cached values until the hard TTL expires. This also lets services that need a break apply backpressure, by telling clients to ignore the soft TTL until the hard TTL expires.

Do not forget to cache negative values, that is: cache error messages or other values associated to failed requests to the target service. Otherwise, a failed target service will not be relieved by the cache if it fails. In addition to caching negative values, implement request coalescing where some logic ensures at most one instance of a request to the target service may be outstanding at any time.

Do test your cached systems to see what happens if the cache fails or requires rebooting (and repopulating) and also the impact of adding and removing cache nodes, as consistent hashing implementations and their properties may differ between cache products. Also be aware of security issues; caches are targets for attackers trying to populate them with values under their control, and the difference between response time for cached and non-cached requests can give observers information (side channel attack).

## Building dashboards for operational visibility - John O'Shea

The article contains wisdom on making good dashboards. Forgoing a summary, I suggest reading a dedicated UX design book instead.

## Avoiding insurmountable queue backlogs -  David Yanacek

Processing failures can quickly lead to large queue backlogs. If just adding extra processing capacity is not enough to clear the backlog quickly then it might be worth using other techniques. For example, depending on the contract with the clients, you may switch queues to LIFO mode when a backlog builds up and transfer part of the backlog to other queues.

## Avoiding fallback in distributed systems -- Jacob Gabrielson

#### [Amazon Builders' Library](https://aws.amazon.com/builders-library)

#### [Shuffle Sharding Implementation](https://github.com/awslabs/route53-infima)

TODO: Add papers from 'Fairness in multi-tenant systems'
